一天爬虫学习日记

学的是最简单的scrapy，有这么好用的工具干嘛不用呢哈哈

爬虫的基本思路：
1、首先给定入口start_urls
2、爬下一级的入口
3、在目标页面爬取所需要的数据

在定义一个spider(crawlspider）类之后，首先给定start_urls
然后就可以开始写回调函数了
一般是写成
def parse(self,response)
函数里面写上请求，可以用一个yield的生成器,request里的callback对于不同层级的页面应该不同
在最终的需要爬取的页面用items存起来，items需要事先在上一级文件里编写好格式
具体提取数据的方法我用的是xpath，对于学校的bbs是挺好用的

最后可以在运行时写入csv文档
scrapy crawl xxxx -o xxx.csv

思路上我是为了找实习，而未名的搜索功能只要进入到特定板块就可以直接搜到全部相关内容，
所以直接把搜索页面作为爬虫入口，且设定时间限制为500天内到帖子，多个入口可以选择自己感兴趣的关键词来设定
然后第一个回调：我们要翻页和进入搜索出来的结果
然后第二个回调：进入之后存下 标题 作者 时间 内容 和帖子链接方便以后查找
存下之后顺便做个去重，这个很容易，在request里面直接设定dont filter=false就可以过滤了
